{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install opencv-python\n",
    "# !nvidia-smi\n",
    "# !pip3 install diffusers\n",
    "!pip3 install diffusers==0.11.1\n",
    "!pip3 install transformers scipy ftfy accelerate\n",
    "!pip3 install torch\n",
    "!pip3 install torch diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize global variable\n",
    "global detected_face_shape\n",
    "detected_face_shape = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detection model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load facial landmark detection model\n",
    "p = \"shape_predictor_68_face_landmarks.dat\"\n",
    "predictor = dlib.shape_predictor(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FACE SHAPE DETECTION PART ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions to calculate distance and ratios:\n",
    "def euclidean_distance(point1, point2):\n",
    "    return math.sqrt((point2[0] - point1[0]) ** 2 + (point2[1] - point1[1]) ** 2)\n",
    "\n",
    "def forehead_width(landmarks):\n",
    "    return euclidean_distance((landmarks.part(2).x, landmarks.part(2).y), (landmarks.part(21).x, landmarks.part(21).y))\n",
    "\n",
    "def face_width(landmarks):\n",
    "    return max(jawline_length(landmarks), forehead_width(landmarks))\n",
    "\n",
    "def face_height(landmarks):\n",
    "    return euclidean_distance((landmarks.part(1).x, landmarks.part(1).y), (landmarks.part(15).x, landmarks.part(15).y))\n",
    "\n",
    "def jawline_length(landmarks):\n",
    "    return euclidean_distance((landmarks.part(0).x, landmarks.part(0).y), (landmarks.part(16).x, landmarks.part(16).y))\n",
    "\n",
    "def cheekbone_length(landmarks):\n",
    "    return euclidean_distance((landmarks.part(10).x, landmarks.part(10).y), (landmarks.part(15).x, landmarks.part(15).y))\n",
    "\n",
    "def ear_length(landmarks):\n",
    "    return euclidean_distance((landmarks.part(4).x, landmarks.part(4).y), (landmarks.part(18).x, landmarks.part(18).y))\n",
    "\n",
    "def interocular_distance(landmarks):\n",
    "    return euclidean_distance((landmarks.part(36).x, landmarks.part(36).y), (landmarks.part(41).x, landmarks.part(41).y))\n",
    "\n",
    "def eye_width(landmarks):\n",
    "    return interocular_distance(landmarks) * 0.5\n",
    "\n",
    "def jaw_width_to_height_ratio(landmarks):return jawline_length(landmarks) / face_height(landmarks)\n",
    "\n",
    "def cheekbones_to_ear_ratio(landmarks):\n",
    "    return cheekbone_length(landmarks) / ear_length(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate_shape function : landmarks\n",
    "def calculate_shape(landmarks):\n",
    "    global detected_face_shape\n",
    "\n",
    "    if 0.85 <= jaw_width_to_height_ratio(landmarks) <= 1.15 and 0.95 <= cheekbones_to_ear_ratio(landmarks) <= 1.05:\n",
    "        detected_face_shape = 'Round'\n",
    "    \n",
    "    elif 0.95 <= jaw_width_to_height_ratio(landmarks) <= 1.05 and abs(face_width(landmarks) - face_height(landmarks)) < 0.1 * face_height(landmarks):\n",
    "        detected_face_shape = 'Square'\n",
    "    \n",
    "    elif 1.25 <= cheekbones_to_ear_ratio(landmarks) <= 1.35 and forehead_width(landmarks) / face_width(landmarks) < 0.45 and jawline_length(landmarks) / face_width(landmarks) > 0.55:\n",
    "        detected_face_shape = 'Diamond'\n",
    "    \n",
    "    else:\n",
    "        detected_face_shape = 'Unknown'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " detect_faces\n",
    "\n",
    "def detect_faces(frame):\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = gray_frame[y:y+h, x:x+w]\n",
    "        landmarks = predictor(face, dlib.rectangle(0, 0, w, h))\n",
    "        shape = calculate_shape(landmarks)\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, detected_face_shape, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def landmarks_for_face(face):\n",
    "    return predictor(face, dlib.rectangle(left=0, top=0, right=face.shape[1], bottom=face.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize camera\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the camera is opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Unable to read camera feed\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture image from the camera\n",
    "ret, start_image = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Unable to capture image from the camera\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the image in a new folder\n",
    "output_folder = \"face_shapes_images\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "image_path = os.path.join(output_folder, \"start_image.jpg\")\n",
    "cv2.imwrite(image_path, start_image)\n",
    "\n",
    "print(\"Start image saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release the camera\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize camera again\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process frame\n",
    "    frame = detect_faces(frame)\n",
    "\n",
    "    cv2.imshow('Face Shapes', frame)\n",
    "\n",
    "    if detected_face_shape is not None:\n",
    "        # Display the detected face shape\n",
    "        cv2.putText(frame, detected_face_shape, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        # print(f\"Detected face shape: {detected_face_shape}\")\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "print(f\"Detected face shape: {detected_face_shape}\")\n",
    "print(\"Camera images processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JEWELS RECOMMENDATION PART ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if detected_face_shape is not None:\n",
    "    # # Define dataset path and shapes folder\n",
    "    dataset_path = \"C:/Users/aditi/OneDrive/Desktop/DESKTOP/VS_LANG/Python/AI/Jewels_dataset\"\n",
    "    d_face_shape = detected_face_shape\n",
    "    shapes_folder = os.path.join(dataset_path, d_face_shape)\n",
    "    # print(\"shapes folder\",shapes_folder)\n",
    "\n",
    "    # Check if the shape folder exists\n",
    "    if not os.path.exists(shapes_folder):\n",
    "        print(f\"The '{d_face_shape}' folder does not exist in the dataset.\")\n",
    "        exit()\n",
    "\n",
    "    jewelry_items=[]\n",
    "    for images in os.listdir(shapes_folder):\n",
    "    \n",
    "        # check if the image ends with png or jpg or jpeg\n",
    "        if (images.endswith(\".png\") or images.endswith(\".jpg\")\\\n",
    "            or images.endswith(\".jpeg\")):\n",
    "            # display\n",
    "            jewelry_items.append(images)\n",
    "\n",
    "    # print(\"jewelery\",jewelry_items)        \n",
    "\n",
    "    # # Select a random jewelry item\n",
    "    selected_jewel = random.choice(jewelry_items)\n",
    "    print(\"selected jewel\",selected_jewel)\n",
    "\n",
    "    if not selected_jewel:\n",
    "        print(f\"No jewelry items available in the '{d_face_shape}/{selected_jewel}' folder.\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMAGE REGENERATION PART ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photograph of an astronaut riding a horse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = pipe(prompt, generator=generator).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
